{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64654f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "  \n",
    "    NULL = u\"\\u0000\"\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(string):\n",
    "        return re.findall(\"\\w+\", string.lower())\n",
    "\n",
    "    @staticmethod\n",
    "    def unique_tokenizer(string):\n",
    "        return set(Tokenizer.tokenize(string))\n",
    "\n",
    "    @staticmethod\n",
    "    def ngram(string, ngram):\n",
    "        tokens = Tokenizer.tokenize(string)\n",
    "\n",
    "        ngrams = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            shift = i - ngram + 1\n",
    "            padding = max(-shift, 0)\n",
    "            first_idx = max(shift, 0)\n",
    "            last_idx = first_idx + ngram - padding\n",
    "            print(i,shift,padding,first_idx,last_idx)\n",
    "\n",
    "            ngrams.append(Tokenizer.pad(tokens[first_idx:last_idx], padding))\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "    @staticmethod\n",
    "    def pad(tokens, padding):\n",
    "        padded_tokens = []\n",
    "\n",
    "        for i in range(padding):\n",
    "            padded_tokens.append(Tokenizer.NULL)\n",
    "\n",
    "        return padded_tokens + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25793f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_string = \"The quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de3bf5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenizer.tokenize(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8664a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -2 2 0 1\n",
      "1 -1 1 0 2\n",
      "2 0 0 0 3\n",
      "3 1 0 1 4\n",
      "4 2 0 2 5\n",
      "5 3 0 3 6\n",
      "6 4 0 4 7\n",
      "7 5 0 5 8\n",
      "8 6 0 6 9\n"
     ]
    }
   ],
   "source": [
    "result = Tokenizer.ngram(new_string, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df6e3f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\x00', '\\x00', 'the'],\n",
       " ['\\x00', 'the', 'quick'],\n",
       " ['the', 'quick', 'brown'],\n",
       " ['quick', 'brown', 'fox'],\n",
       " ['brown', 'fox', 'jumps'],\n",
       " ['fox', 'jumps', 'over'],\n",
       " ['jumps', 'over', 'the'],\n",
       " ['over', 'the', 'lazy'],\n",
       " ['the', 'lazy', 'dog']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fefbca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c221e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTokenizer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.string = \"this is a test of the emergency broadcasting system\"\n",
    "\n",
    "    def test_downcasing(self):\n",
    "        expectation = [\"this\", \"is\", \"all\", \"caps\"]\n",
    "\n",
    "        actual = Tokenizer.tokenize(\"THIS IS ALL CAPS\")\n",
    "        self.assertEqual(actual, expectation,\"Wrong\")\n",
    "\n",
    "    def test_ngrams(self):\n",
    "        expectation = [\n",
    "          [u'\\u0000', \"quick\"],\n",
    "          [\"quick\", \"brown\"],\n",
    "          [\"brown\", \"fox\"],\n",
    "        ]\n",
    "\n",
    "        actual = Tokenizer.ngram(\"quick brown fox\", 2)\n",
    "        self.assertEqual(actual, expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2d14ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TestTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c43b1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.test_downcasing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8e855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
